# GNSS 项目

## 1. 数据处理

### 1.1 处理过程

**任务目标：**将原始数据转换成可以被模型读入的数据。



模型读入数据分成两部分：

#### **a. 观测站数据**

**来源：**pos_feature文件夹，roti_feature文件夹

> 文件夹下每个case文件夹代表一个时间段，case文件夹下每个文件代表一个观测站的数据（不同case中观测站的数量从17~20个不等）

**处理方式：**

1. 将pos_features和roti_features对应的case文件中各个观测站数据按特征维度拼接；将每个case处理成表示时间序列的三维矩阵形式；矩阵各维度含义为：[timestep, sensors, features]；最终17个case分别得到17个case*.npy文件。分别对每个sensor的每个feature分别做归一化操作（减去均值，除以标准差）

> timestep代表时间戳，相邻timestep时间间隔为30s；
>
> sensors代表不同观测站，不同case数量17~20个不等
>
> features代表不同特征，长度11，
>
> 特征按顺序为【average_roti **| **median_roti **|** average(abs(rot)) **|** median(abs(rot) **|** X **|** Y **|** Z **|** ΔX **|** ΔY **|** ΔZ **|** ΔE^2 + ΔN^2 + ΔU^2 】
>
> **漏洞：**没有保证pos_feature和rotifeatures按特征拼接时观测站之间是一一对应的



2. 对于每个case*.npy文件，通过滑动窗口操作获得模型所需要的数据：

> global_attn_state: [数据条数，sensors, features, time_steps]
>
> global_inputs: [数据条数，time_steps, sensors] 即对global_attn_state只取目标属性( ΔE^2 + ΔN^2 + ΔU^2 )的数据，features维度消失；
>
> local_inputs: [数据条数，time_steps, features] 即对global_attn_state只取目标观测站(随机选择了一个观测站)的数据，sensors维度消失）
>
> decoder_gts: [数据条数，1] 每条数据对应的标签，这条数据未来某个时间步对应的ΔE^2 + ΔN^2 + ΔU^2特征取值>1 , 则标签为1（有影响）， <1, 则标签为0（无影响）



**最终处理结果：**将pos_feature和roti_feature文件夹中每个case分别合并生成能输入模型的case文件。

**细节问题与目前处理方式：**

1. 将原来数据的ΔE, ΔN, ΔU三个特征转换成ΔE^2 + ΔN^2 + ΔU^2 一个特征，存在一定程度信息丢失；同时目标属性没有给出，暂设为ΔE^2 + ΔN^2 + ΔU^2。

2. 存在时间不连续的情况，用与该时间邻近的时间的取值填充该时间的取值；（示例）
3. 存在pos_feature 和roti_feature对应case文件夹中观测站数目不匹配的情况，没有对这些case做数据处理，因为pos_feature和roti_feature不好合并；
4. 没有给出目标观测站，任取一个观测站作为要预测的观测站；



#### b.外部特征数据

**来源：**External features文件夹（没有使用No.7(2)）

**处理方式：**同样通过滑动窗口操作得到模型能读入的数据 【数据条数，特征】；与观测站数据是一一对应的（选取每条观测站数据中最后一个时间戳对应的外部特征数据作为对应输入）

**细节问题与目前处理方式：**

1. external features中异常数据(e.x. 999.99)和缺失数据都通过前向填充或后向填充来处理。(举例)
2. 对于每条输入数据，只选取最后一个时间戳对应的external feature作为输入。



>  其他问题：
>
> 1. 是否要用GNSS观测站之间的距离信息？



## 2. 模型架构

**目前进展：**让模型架构适配“**单时间步的二分类预测问题**”，搭建一个demo

模型修改的部分：

1. temporal fusion使用**多头自注意力机制**，并序列输出求和；求和后得到的向量与external feature拼接后通过一个全连接层得到一个预测值，并通过sigmoid函数得到一个0-1之间的预测值。
2. loss函数使用交叉熵损失（暂未尝试论文中提到的focal loss）



## 3. 初步实验

只选择一个case（case14）的数据训练；

共4958条数据；3838条数据标签为0（无影响）1120条数据标签为1（有影响）；**数据存在不均衡**

80%作为训练集（其中987条标签为1）；20%作为测试集（133条标签为1）；



训练结果：在测试集上的准确率从82.7%提高到90.3%。一开始准确率高可能是因为数据不均衡；



目前模型参数少，数据量小，仅在CPU上跑大约十分钟就可以完成实验。



## 3. 下一步规划

数据层面：

1. 要将不同case的数据混合在一起，关键问题是不同case中观测站数量不一致；一个简单粗暴的方式是直接舍弃存在缺失的观测站数据；
2. 解决几个疑点：目标属性；目标观测站；异常数据和缺失数据的填充策略；pos features和roti features是否可以简单拼接；ΔE, ΔN, ΔU三个特征是否可以转换成ΔE^2 + ΔN^2 + ΔU^2 一个特征；阈值；时间步长取值；每条数据的external feature取法；



模型训练层面：

1. external feature 的处理，不是直接拿原始数据拼接，可以先做一个embedding操作
2. 采用更有效的评价指标：recall，precision，f1；
3. 代码重构：dataset、dataloader、spatial attention代码；

3. 使用focal loss，处理不平衡的数据集；
4. 其他超参数调整；
5. 模型架构调整：加大加深网络（在数据量大了以后）





## 4. 需要修改的细节

时间间隔一分钟；

F10.7特征不要；

目标传感器和目标特征：

没有所有case的传感器的数据去掉；

预测步长参考论文



有没有观测站之间的空间位置关系？
